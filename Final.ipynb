{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import (metrics, model_selection, linear_model, preprocessing, ensemble, neighbors, tree)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint as pp\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from xgboost import XGBClassifier,plot_tree,to_graphviz\n",
    "from IPython.display import Image \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import txdot_parse as txpars\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the \"crash\" data\n",
    "data = pd.read_csv(\"my_map_grid.csv\",header=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# lowercase: http://stackoverflow.com/a/38931854\n",
    "data.columns = data.columns.str.lower()\n",
    "# remove spaces\n",
    "data.columns = data.columns.str.replace(' ', '_')\n",
    "# special cases\n",
    "data.columns = data.columns.str.replace('crash_i_d', 'crash_id')\n",
    "# remove whateva data\n",
    "\n",
    "# replace ['No Data','Not Applicable'] with NaN\n",
    "data.replace(to_replace='No Data', value=np.nan, inplace=True)\n",
    "data.replace(to_replace='Not Applicable', value=np.nan, inplace=True)\n",
    "data.latitude = pd.to_numeric(data.latitude)\n",
    "data.longitude = pd.to_numeric(data.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "pre : total missing speed limit data:\n",
      " 575 (0.2576164874551971 of 1)\n",
      "post: total missing speed limit data:\n",
      " 458 (0.20519713261648745 of 1)\n",
      "########################################\n"
     ]
    }
   ],
   "source": [
    "data.speed_limit.replace(0,np.nan,inplace=True)\n",
    "data.speed_limit.replace(-1,np.nan,inplace=True)\n",
    "# impute speed limits\n",
    "print(\"########################################\")\n",
    "verbose=0\n",
    "if(verbose):\n",
    "    print(data.speed_limit.value_counts())\n",
    "totalmissing   = data[data['speed_limit'].isnull()].shape[0]\n",
    "missingpercent = totalmissing / data.shape[0]\n",
    "print(\"pre : total missing speed limit data:\\n %s (%s of 1)\" % (totalmissing, missingpercent))\n",
    "if(verbose):\n",
    "    print(data.speed_limit.unique())\n",
    "data = txpars.impute_mph(data, verbose=0)\n",
    "totalmissing   = data[data['speed_limit'].isnull()].shape[0]\n",
    "missingpercent = totalmissing / data.shape[0]\n",
    "print(\"post: total missing speed limit data:\\n %s (%s of 1)\" % (totalmissing, missingpercent))\n",
    "if(verbose):\n",
    "    print(data.speed_limit.unique())\n",
    "    print(data.speed_limit.value_counts())\n",
    "    print(data.info())\n",
    "print(\"########################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import (metrics, model_selection, linear_model, preprocessing, ensemble, neighbors)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint as pp\n",
    "import re\n",
    "#import xgboost as xgb\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the realationship between crashes and time of day\n",
    "# First, let's just look at the total crashes vs time of day (no crash severity)\n",
    "\n",
    "data = pd.read_csv(\"my_map_grid.csv\",header=10)\n",
    "data.columns = data.columns.str.lower()\n",
    "# remove spaces\n",
    "data.columns = data.columns.str.replace(' ', '_')\n",
    "\n",
    "# add \n",
    "ax_time = helpers.get_ax_time(\n",
    "        interval = '24h',\n",
    "        title = 'Frequency of Bike Crashes For Time of Day (2010-2017)',\n",
    "        xlabel = 'Time of Day (24 hr)',\n",
    "        ylabel = 'count',\n",
    "        )\n",
    "\n",
    "data.crash_time.hist(bins=48,ax=ax_time)\n",
    "plt.show()\n",
    "\n",
    "# Let's see if there is a trend of total bike crashes by year\n",
    "# We'll ignore 2017 data since it is incomplete\n",
    "data.crash_year[data.crash_year != 2017].plot.hist(bins=7)\n",
    "plt.xlabel('Year')\n",
    "plt.title('Frequency of Bike Crashes Per Year (2010-2016)')\n",
    "plt.show()\n",
    "\n",
    "# Spread of Crash Severity data from 2010-2017\n",
    "# Using seaborn for ease of use with categorical data\n",
    "sns.countplot(x=\"crash_severity\", data=data);\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Crash Severity Total Count (2010-2017)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'Wet' 'Dry' to '1' '0'\n",
    "data['surface_condition'] = data['surface_condition'].factorize()[0]\n",
    "# DOC: rename col http://stackoverflow.com/a/11346337\n",
    "data.rename(columns={'surface_condition':'surface_wet'})\n",
    "# print number of unique\n",
    "for colname in data.columns:\n",
    "    print(\"% 4d : %s\" % (len(data[colname].unique()), colname))\n",
    "# remove data which is has no importance\n",
    "# better to drop cols with all NaN and convert \"unimportant\" data to NaN\n",
    "#  - can't universally decide to drop col just based on uniqueness\n",
    "# e.g. all of object_struck is 'Not Applicable' and useless, but if surface_condition had only one value \"dry\" this would be important\n",
    "# ? for colname in data.columns:\n",
    "# colname = 'object_struck'\n",
    "# if(len(data[colname].unique()) == 1):\n",
    "#   print(\"-I-: dropping %s for having all homogenous values %s\", (colname, data[colname].unique()[0]))\n",
    "#   data.drop(colname,axis=1,inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "if(1):\n",
    "  data.describe()\n",
    "  data.hist()\n",
    "  data.corr().plot() # TODO: seaborn\n",
    "  plt.show()\n",
    "else:\n",
    "  print(\"-I-: Skipping...\")\n",
    "\n",
    "pairplot_var_list = [\n",
    "# 'crash_id',\n",
    " 'average_daily_traffic_amount',\n",
    " 'average_daily_traffic_year',\n",
    " 'crash_death_count',\n",
    "# 'crash_incapacitating_injury_count',\n",
    "# 'crash_non-incapacitating_injury_count',\n",
    "# 'crash_not_injured_count',\n",
    "# 'crash_possible_injury_count',\n",
    " 'crash_severity',\n",
    " 'crash_time',\n",
    " 'crash_year',\n",
    " 'day_of_week',\n",
    "# 'intersecting_street_name',\n",
    " 'intersection_related',\n",
    "# 'latitude',\n",
    " 'light_condition',\n",
    "# 'longitude',\n",
    " 'manner_of_collision',\n",
    " 'medical_advisory_flag',\n",
    " 'number_of_entering_roads',\n",
    " 'number_of_lanes',\n",
    "# 'object_struck',\n",
    " 'road_base_type',\n",
    " 'speed_limit',\n",
    "# 'street_name',\n",
    " 'surface_condition'\n",
    " ]\n",
    "\n",
    "dummies_needed_list = [\n",
    " 'crash_severity',\n",
    " 'day_of_week',\n",
    " 'intersection_related',\n",
    " 'light_condition',\n",
    " 'manner_of_collision',\n",
    " 'number_of_entering_roads',\n",
    " 'road_base_type',\n",
    "# 'surface_condition' # factorized\n",
    "        ]\n",
    "\n",
    "# tmp disable\n",
    "if(0):\n",
    "    sns.pairplot(data, vars=pairplot_var_list)\n",
    "    plt.show()\n",
    "\n",
    "# alternative visualisation\n",
    "datapt = data.pivot_table(values=['crash_death_count','crash_incapacitating_injury_count','crash_non-incapacitating_injury_count'], index=['speed_limit','crash_time'])\n",
    "print(datapt)\n",
    "\n",
    "pp.pprint(list(pd.get_dummies(data[dummies_needed_list]).columns))\n",
    "pp.pprint(list(pd.get_dummies(data[dummies_needed_list]).columns.str.replace('[,\\s]+','_').str.lower()))\n",
    "'''\n",
    " 'Dark, Lighted', 'dark_lighted_yes'\n",
    " 'Dark, Not Lighted', 'dark_lighted_no'\n",
    " 'Dark, Unknown Lighting', 'dark_lighted_unknown'\n",
    " 'Dawn',\n",
    " 'Daylight',\n",
    " 'Dusk',\n",
    " 'Unknown',\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print important features\n",
    "xgb_imp_feats = helpers.print_model_feats_important(model_xgb, x_train.columns)\n",
    "ax = helpers.get_ax_barh(xgb_imp_feats, title=\"DecisionTree Important Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: run model in one cell, interpret in another. that way model isn't re-run each time the cell is changed\n",
    "# DOC: How to interpret decision trees' graph results and find most informative features?\n",
    "# src: http://stackoverflow.com/a/34872454\n",
    "print(\"-I-: most important features:\")\n",
    "def print_model_feats_important(model, predictors):\n",
    "    for i in np.argsort(model.feature_importances_)[::-1]:\n",
    "      if model.feature_importances_[i] == 0:\n",
    "        continue\n",
    "      print(\"%f : %s\" % (model.feature_importances_[i],predictors[i]))\n",
    "print_model_feats_important(model_xgb, x_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
